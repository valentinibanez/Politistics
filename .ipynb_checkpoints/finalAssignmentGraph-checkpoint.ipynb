{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# encoding: utf-8\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import numpy as np\n",
    "import math\n",
    "from glob import glob\n",
    "import fileinput\n",
    "import re\n",
    "import collections\n",
    "import pickle \n",
    "from alphabet_detector import AlphabetDetector\n",
    "import os\n",
    "import seaborn as sns\n",
    "import community\n",
    "from collections import deque\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Politistics\n",
    "Political alienation is a recurring theme in todays politics, recently we witnessed it playing a role in the American election as well as the british _Brexit_. Paradoxically we are in an age where polticians can connect to the voters in many new ways, and we saw social media, like Twitter, being widely used in the recent american election.\n",
    "\n",
    "In this report we will be analysing how politicians use social media, who they talk about, what they talk about and how they talk about them. It is difficult to say what is the cause of polticial alienation, but smear campaigns and negative rhetorics amongst polticians could be a symptom as well as a cause, other factors could be at play but will not be the focus of this project. \n",
    "\n",
    "We will be using graph theory to both visualize and analyse our data as well as text analysis, namely tf-idf and word sentiment to analyse the contents of tweets. Our goal is to try to shed some light on how Twitter is used on the political stage, such as who thay talk about and how they talk about each other.\n",
    "\n",
    "## The Data\n",
    "By using [tweepy](http://www.tweepy.org/) we have collected the last 3400 tweets from 514 politicians on Twitter found [here](http://www.tweepy.org/). By scraping and cross referencing names from [this](http://www.danskepolitikere.dk/) website and manually finding and entering politicial parties, we have been able to identify the party of 234 of the 514 politicians. This has enabled us to construct directed graphs, using politicians Twitter accounts as nodes and referrences to other politicians as edges in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>twitterAccount</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>tweets</th>\n",
       "      <th>created_at</th>\n",
       "      <th>type</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Margrethe Vestager</td>\n",
       "      <td>@vestager</td>\n",
       "      <td>172234</td>\n",
       "      <td>280</td>\n",
       "      <td>5318</td>\n",
       "      <td>20/01-2009</td>\n",
       "      <td>EU</td>\n",
       "      <td>Radikale Venstre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lars Løkke Rasmussen</td>\n",
       "      <td>@larsloekke</td>\n",
       "      <td>80089</td>\n",
       "      <td>302</td>\n",
       "      <td>926</td>\n",
       "      <td>24/03-2009</td>\n",
       "      <td>Folketinget</td>\n",
       "      <td>Venstre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Søren Pind</td>\n",
       "      <td>@sorenpind</td>\n",
       "      <td>48086</td>\n",
       "      <td>326</td>\n",
       "      <td>9076</td>\n",
       "      <td>25/10-2007</td>\n",
       "      <td>Folketinget</td>\n",
       "      <td>Venstre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ida Auken</td>\n",
       "      <td>@IdaAuken</td>\n",
       "      <td>39745</td>\n",
       "      <td>1979</td>\n",
       "      <td>8588</td>\n",
       "      <td>12/03-2009</td>\n",
       "      <td>Folketinget</td>\n",
       "      <td>Radikale Venstre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morten Østergaard</td>\n",
       "      <td>@oestergaard</td>\n",
       "      <td>36471</td>\n",
       "      <td>736</td>\n",
       "      <td>6008</td>\n",
       "      <td>20/01-2009</td>\n",
       "      <td>Folketinget</td>\n",
       "      <td>Radikale Venstre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Henning Tønning</td>\n",
       "      <td>@Tnning</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>07/09-2012</td>\n",
       "      <td>Regionalt/lokalt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Holger Gorm Petersen</td>\n",
       "      <td>@gorm_hogpe</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>04/12-2013</td>\n",
       "      <td>Regionalt/lokalt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>Carsten Abild</td>\n",
       "      <td>@carstenabild</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21/06-2012</td>\n",
       "      <td>Regionalt/lokalt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Arne Kristiansen</td>\n",
       "      <td>@ArneBoelt</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>30/11-2013</td>\n",
       "      <td>Regionalt/lokalt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Hans Thorup</td>\n",
       "      <td>@HansThorup</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>26/11-2013</td>\n",
       "      <td>Regionalt/lokalt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name twitterAccount  followers  following  tweets  \\\n",
       "0      Margrethe Vestager      @vestager     172234        280    5318   \n",
       "1    Lars Løkke Rasmussen    @larsloekke      80089        302     926   \n",
       "2              Søren Pind     @sorenpind      48086        326    9076   \n",
       "3               Ida Auken      @IdaAuken      39745       1979    8588   \n",
       "4       Morten Østergaard   @oestergaard      36471        736    6008   \n",
       "..                    ...            ...        ...        ...     ...   \n",
       "513       Henning Tønning        @Tnning         11         10       0   \n",
       "514  Holger Gorm Petersen    @gorm_hogpe         10          1       0   \n",
       "515         Carsten Abild  @carstenabild         10          0       0   \n",
       "516      Arne Kristiansen     @ArneBoelt          9         17       0   \n",
       "517           Hans Thorup    @HansThorup          6          5       0   \n",
       "\n",
       "     created_at              type             Party  \n",
       "0    20/01-2009                EU  Radikale Venstre  \n",
       "1    24/03-2009       Folketinget           Venstre  \n",
       "2    25/10-2007       Folketinget           Venstre  \n",
       "3    12/03-2009       Folketinget  Radikale Venstre  \n",
       "4    20/01-2009       Folketinget  Radikale Venstre  \n",
       "..          ...               ...               ...  \n",
       "513  07/09-2012  Regionalt/lokalt               NaN  \n",
       "514  04/12-2013  Regionalt/lokalt               NaN  \n",
       "515  21/06-2012  Regionalt/lokalt               NaN  \n",
       "516  30/11-2013  Regionalt/lokalt               NaN  \n",
       "517  26/11-2013  Regionalt/lokalt               NaN  \n",
       "\n",
       "[518 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in word valence dict\n",
    "sentimentDict = {}\n",
    "fnames = glob('AFINN-*.txt')\n",
    "for line in fileinput.FileInput(fnames):\n",
    "    words = line.split('\\t')\n",
    "    word = words[0]\n",
    "    sentimentDict[word] = words[1]\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('politiciansFull.csv')\n",
    "df2 = pd.read_csv('politiciansPartiesUtf.csv')\n",
    "\n",
    "fullDf = pd.merge(df1, df2, how='left', on=['name'])\n",
    "# using the left right spectrum collected from http://www.altinget.dk/artikel/det-nye-politiske-kompas\n",
    "# we have indexed every politican with a corresponding index on the left right scale \n",
    "\n",
    "parties = [\"Enhedslisten\", 'Alternativet', \"Socialistisk Folkeparti\", \n",
    "           \"Socialdemokraterne\", 'Radikale Venstre', 'Venstre', \n",
    "           'Det Konservative Folkeparti', 'Liberal Alliance', 'Dansk Folkeparti'] \n",
    "\n",
    "colorCoding = {'Venstre':'b', 'Socialdemokraterne': 'r', 'Radikale Venstre': 'm', \n",
    "               'Enhedslisten':'#9F000F', 'Socialistisk Folkeparti':'#C24641', 'Liberal Alliance':'#82CAFF', \n",
    "               'Dansk Folkeparti':'#FFD801', 'Det Konservative Folkeparti': '#347C17', 'Alternativet': '#00FF00'}\n",
    "\n",
    "fullDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.590962 MB\n"
     ]
    }
   ],
   "source": [
    "def get_size(start_path = '.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "print str(get_size('cleaned_politikere_tweets')/1000000.0) + \" MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Basic Network Statistics\n",
    "From the collected data we have built two different networks, one which is a directed graph and one which is a directed multi graph. The two have different advantages, the regular graph allows for more analysis to be performed by libraries such as _community_, while the multi graph is a better model for the information, since it models the refences as edges in one-to-one.\n",
    "\n",
    "The following functions are used for generating the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add politicians as nodes, depending on their party, then give them\n",
    "# an index on the left-right political spectrum and and a poltical block\n",
    "def loadInNodes(G):\n",
    "    for index, row in fullDf.iterrows():\n",
    "        if pd.isnull(row['Party']):\n",
    "            G.add_node(row['twitterAccount'], \n",
    "                       {'name': row['name'], 'followers': row['followers'], \n",
    "                        'following': row['following'], 'tweets': row['tweets'], \n",
    "                        'created_at':row['created_at'], 'type': row['type'], 'party': 'None'})\n",
    "        else:\n",
    "            psIndex = -1\n",
    "            block = -1\n",
    "            if row['Party'] == 'Venstre':\n",
    "                block = 1\n",
    "                psIndex = 7\n",
    "            elif row['Party'] == 'Socialdemokraterne':\n",
    "                psIndex =  4\n",
    "                block = 0\n",
    "            elif row['Party'] == 'Radikale Venstre':\n",
    "                psIndex = 1\n",
    "                block = 0\n",
    "            elif row['Party'] == 'Enhedslisten':\n",
    "                psIndex = 2\n",
    "                block = 0\n",
    "            elif row['Party'] == 'Socialistisk Folkeparti':\n",
    "                psIndex = 3\n",
    "                block = 0\n",
    "            elif row['Party'] == 'Liberal Alliance':\n",
    "                psIndex = 6\n",
    "                block = 1\n",
    "            elif row['Party'] == 'Dansk Folkeparti':\n",
    "                psIndex = 8\n",
    "                block = 1\n",
    "            elif row['Party'] == 'Det Konservative Folkeparti':\n",
    "                psIndex = 5\n",
    "                block = 1\n",
    "            elif row['Party'] == 'Alternativet':\n",
    "                psIndex = 0\n",
    "                block = 0\n",
    "            G.add_node(row['twitterAccount'], \n",
    "                       {'name': row['name'], 'followers': row['followers'], \n",
    "                        'following': row['following'], 'tweets': row['tweets'], \n",
    "                        'created_at':row['created_at'], 'type': row['type'], \n",
    "                        'party': row['Party'], 'politicalSpectrum': psIndex, 'Block': block})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load in edges based on each reference made \n",
    "# from a poltician to another politician.\n",
    "# if a reference is found the the sentiment profile of\n",
    "# the tweet is also stored in the edge for later\n",
    "# if the graph is not a multi graph \n",
    "# then it adds weight to edges\n",
    "def loadInEdges(G):\n",
    "    for index, row in fullDf.iterrows():\n",
    "        foundCSV = False\n",
    "        try:\n",
    "            tweets = pd.read_csv('cleaned_politikere_tweets/' + \n",
    "                                 row['twitterAccount'][1:] + \".csv\")\n",
    "            foundCSV = True\n",
    "        except:\n",
    "            pass\n",
    "        if foundCSV:\n",
    "            for tweet in tweets['text']:\n",
    "                for politician in fullDf['twitterAccount']:\n",
    "                    if str(politician) in tweet:\n",
    "                        sentiment = 0\n",
    "                        words = tweet.replace(',','').replace('.','').split(' ')\n",
    "                        count = 0\n",
    "                        for word in words:\n",
    "                            if word in sentimentDict:\n",
    "                                sentiment += int(sentimentDict[word])\n",
    "                                count += 1\n",
    "                        if count != 0:\n",
    "                            sentiment = sentiment/float(count)\n",
    "                        else:\n",
    "                            sentiment = None\n",
    "                        if type(G) == nx.DiGraph:\n",
    "                            if G.has_edge(row['twitterAccount'],politician):\n",
    "                                if sentiment != None:\n",
    "                                    if G.edge[row['twitterAccount']][politician]['sentiment'] != None:\n",
    "                                        G.edge[row['twitterAccount']][politician]['sentiment'] += int(sentiment)\n",
    "                                    else:\n",
    "                                        G.edge[row['twitterAccount']][politician]['sentiment'] = int(sentiment)\n",
    "                                G.edge[row['twitterAccount']][politician]['weight'] += 1\n",
    "                            else:\n",
    "                                G.add_edge(row['twitterAccount'], politician, sentiment=sentiment, weight=1)\n",
    "                        elif type(G) == nx.MultiDiGraph:\n",
    "                            G.add_edge(row['twitterAccount'], politician, sentiment=sentiment)\n",
    "    # remove all nodes that are not connected to any other poltician\n",
    "    G.remove_nodes_from(nx.isolates(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# draw the given graph, amount of followers dictates size\n",
    "# of node and party determines color\n",
    "def drawPoliGraph(G, alpha):\n",
    "    plt.figure(1, figsize=(15,15))\n",
    "    pos=nx.spring_layout(G, k=0.9)\n",
    "    for node in G.nodes():\n",
    "        color = 'w'\n",
    "        if G.node[node]['party'] in colorCoding:\n",
    "            color = colorCoding[G.node[node]['party']]\n",
    "        else:\n",
    "            color = '#999966'\n",
    "            \n",
    "        nx.draw_networkx_nodes(G,pos,\n",
    "                           nodelist=[node],\n",
    "                           node_color=color,\n",
    "                           node_size=G.node[node]['followers']/80 + 5,alpha=0.8)\n",
    "        if G.node[node]['followers']>20000:\n",
    "            nx.draw_networkx_labels(G, pos, font_size= 8,font_color='w',labels= {node.decode(\"utf-8\"):G.node[node]['name'].decode(\"utf-8\")})\n",
    "    nx.draw_networkx_edges(G, pos, arrows=False, width=0.3, alpha=alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate and plot the two different graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "loadInNodes(G)\n",
    "loadInEdges(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiG = nx.MultiDiGraph()\n",
    "loadInNodes(multiG)\n",
    "loadInEdges(multiG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawPoliGraph(G, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawPoliGraph(multiG,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both graphs contains 455 nodes but the directed graph only contains 18484 edges whereas the multi graph contains 158845."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Directed Graph Edges: \" + str(len(G.edges()))\n",
    "print \"Directed Multi Graph Edges: \" + str(len(multiG.edges()))\n",
    "\n",
    "print \"Directed Graph Nodes:\" + str(len(G.nodes()))\n",
    "print \"Directed Multi Graph Nodes:\" + str(len(multiG.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By having a look at the degree distribution of the multigraph, we can see like most social networks it follows the degree distribution of a scale free network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GDegrees = G.degree(G.nodes()).values()\n",
    "Dmax = np.amax(GDegrees)\n",
    "Dmin = np.amin(GDegrees)\n",
    "hist, bins = np.histogram(GDegrees, range(Dmin, (Dmax + 1)))\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(221)\n",
    "plt.plot(center, hist, 'ro', markersize=4)\n",
    "plt.title('distribution of degree in the politicians network')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.loglog(center, hist, 'ro', markersize=4)\n",
    "plt.title('distribution of degree in the politicians network on a log scale')\n",
    "\n",
    "GDegrees = multiG.degree(multiG.nodes()).values()\n",
    "Dmax = np.amax(GDegrees)\n",
    "Dmin = np.amin(GDegrees)\n",
    "hist, bins = np.histogram(GDegrees, range(Dmin,(Dmax + 1)))\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.subplot(223)\n",
    "plt.plot(center, hist, 'ro', markersize=4)\n",
    "plt.title('distribution of degree in a multi graph polticians network')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.loglog(center, hist, 'ro', markersize=4)\n",
    "plt.title('distribution of degree in a multi graph polticians network on a log scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Especially the directed multi graph seems to follow a power law destribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print mostBetweenPoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are they talking about?\n",
    "In this section we will be covering what the politicians are talking about on twitter. First we will be describing what measures have been taken to clean and prepare the data, as well as which methods we have been using to analyse differences in word patterns between parties.\n",
    "### Preparing the Text Data \n",
    "Some of our preliminary attempts at plotting the word clouds resultet in some strange plots. Like the following word cloud plot of the tf-idf from the top 400 words used by _Dansk Folkeparti_.\n",
    "![title](Pictures/gamingPolitiker.png)\n",
    "By searching through the tweets of members of _Dansk Folkeparti_, we found out that one of the members of the parti had connected her twitter account to her player account in an ipad game. The game was then auto tweeting her accomplishments in the game, this was going on for some weeks, resulting in an abundance of words like _game_, _tribez_, _coins_, etc.\n",
    "A similar case was in the following word cloud plot.\n",
    "![title](Pictures/runningPolitiker.png)\n",
    "Where a poltician had connected her endomondo runner application to her twitter profile, causing the words _running_, _here:_ and _km_ to show up.\n",
    "Our solution to this problem was to create a cleaner function (Appendix D) that through a key identifier, like, '#Endomondo' or 'tribez' finds lines in csv files and deletes them.\n",
    "\n",
    "Another issue that we encountered was utf encoded words that could not be displayed by the word cloud module. This was the case with words which contained non latin charachters, so we removed them when creating the word counts by using the alphabet-detector library. But some charachters stil poses problem when trying to plot them and are stil a part of the text data. These include the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array = [\n",
    "    sorted(convertToLoT(calcTfIdf(BoWsDict['Socialdemokraterne'], BoWsDict, 400)), key=lambda x: x[1], reverse=True)[1][0],\n",
    "    sorted(convertToLoT(calcTfIdf(BoWsDict['Socialdemokraterne'], BoWsDict, 400)), key=lambda x: x[1], reverse=True)[14][0],\n",
    "    sorted(convertToLoT(calcTfIdf(BoWsDict['Socialistisk Folkeparti'], BoWsDict, 400)), key=lambda x: x[1], reverse=True)[3][0],\n",
    "    sorted(convertToLoT(calcTfIdf(BoWsDict['Socialistisk Folkeparti'], BoWsDict, 400)), key=lambda x: x[1], reverse=True)[7][0],\n",
    "    sorted(convertToLoT(calcTfIdf(BoWsDict['Socialistisk Folkeparti'], BoWsDict, 400)), key=lambda x: x[1], reverse=True)[7][0],\n",
    "    sorted(convertToLoT(calcTfIdf(BoWsDict['Liberal Alliance'], BoWsDict, 400)), key=lambda x: x[1], reverse=True)[16][0]\n",
    "]\n",
    "print \" \".join(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word counts and sentiment profiles are calculated using a [MRJob](https://pythonhosted.org/mrjob/) script. These scripts ([appendix A](#Appendix-A) and [Appendix E]((#Appendix-E)) use a mapreduce technique to perform operations which makes them very quick, but they are not well suited for csv files. Therefore we had to implement a cleaner.sh file [appendix C](#Appendix-C) which removed new lines within rows of the csv file. This pattern recognition as well as most others in the project is done using regular expressions.\n",
    "\n",
    "The resulting word count dictionaries of all the politicians of each party are dumped as pickle files.\n",
    "The following functions are used for retreiving the word counts, calculating the _term frequency-inverse document frequency_ (tf-idf) and plotting them in a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in pickle word count dictionary for each party\n",
    "# seperate words into hastags, references and regular words.\n",
    "def loadInSepOcc(BoWsDict, RefsDict, hashtagsDict, path, minOcc = 0):\n",
    "    for party in parties:\n",
    "        refDict = collections.OrderedDict()\n",
    "        hashDict = collections.OrderedDict()\n",
    "        wordDict = collections.OrderedDict()\n",
    "        BoW = pickle.load( open(path + \"/\" + party +  \"WordCount.p\", \"rb\" ) )\n",
    "        for k,v in BoW.items():\n",
    "            if v >= minOcc:\n",
    "                if len(k) > 0:\n",
    "                    if k[0]=='#':\n",
    "                        hashDict[k] = v\n",
    "                    elif k[0]=='@':\n",
    "                        refDict[k] = v\n",
    "                    else:\n",
    "                        wordDict[k] = v\n",
    "        BoWsDict[party] = wordDict\n",
    "        RefsDict[party] = refDict\n",
    "        hashtagsDict[party] = hashDict\n",
    "        print \"done \" + party +\" and length of bow was \" + str(len(wordDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcTfIdf(BoW, BoWList, topAmount =  0):\n",
    "    if topAmount==0:\n",
    "        topAmount = len(BoW)\n",
    "    tfIdfDict = {}\n",
    "    # calculate total amount of words in order to calculate the frequncy\n",
    "    total = float(sum(BoW.values()))\n",
    "    # filter of all but a top amount of words\n",
    "    words = list(BoW)[:topAmount]\n",
    "    Dictionaries = []\n",
    "    # build a dictionary of the needed words for fast lookup\n",
    "    for D in BoWList.values():\n",
    "        if topAmount != len(BoW):\n",
    "            Dictionaries.append(dict((k, D[k]) for k in list(D)[:topAmount]))\n",
    "        else:\n",
    "            Dictionaries.append(BoW)\n",
    "    # calculate the tf-idf for the top n most used words in the bag of words\n",
    "    for word in words:\n",
    "        n = sum(1 for x in Dictionaries if word in x)\n",
    "        tf =  BoW[word] / total\n",
    "        idf = math.log((len(BoWList) + 1) / float(1 + n))\n",
    "        tfIdfDict[word] = tf * idf\n",
    "    return tfIdfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToLoT(dictionary):\n",
    "    # convert to list of tuples for plotting\n",
    "    LoT = []\n",
    "    for word, value in dictionary.items():\n",
    "        if not word.lower() in stopWords.union(STOPWORDS):\n",
    "            LoT.append((word.decode('utf-8'), value))\n",
    "    return LoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.misc import imread\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(%d, 100%%, 50%%)\" % random.randint(0, 270)\n",
    "\n",
    "def plotWordCloud(TuplelistWord, TuplelistRef, TuplelistTag, party):\n",
    "    # plot a wordcloud of the BoW\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(15,30))\n",
    "    cloudMask = imread(\"Pictures/cloudMask.png\")\n",
    "    birdMask = imread(\"Pictures/twitter_mask.png\")\n",
    "    cloudMask2 = imread(\"Pictures/mask-cloud.png\")\n",
    "    splatMask = imread(\"Pictures/spot-splatter-stencil.jpg\")\n",
    "    wcWords = WordCloud(background_color=\"white\", max_words=40, mask=cloudMask2)\n",
    "    wcWords.generate_from_frequencies(TuplelistWord)\n",
    "    axarr[0].imshow(wcWords.recolor(color_func=color_func, random_state=3))\n",
    "    axarr[0].axis(\"off\")\n",
    "    \n",
    "    wcRefs = WordCloud(background_color=\"white\", max_words=40, mask=cloudMask2)\n",
    "    wcRefs.generate_from_frequencies(TuplelistRef)\n",
    "    axarr[1].imshow(wcRefs.recolor(color_func=color_func, random_state=3))\n",
    "    axarr[1].axis(\"off\")\n",
    "    axarr[1].set_title(party, fontsize=20, fontweight='bold')\n",
    "    \n",
    "    wcTags = WordCloud(background_color=\"white\", max_words=40, mask=cloudMask2)\n",
    "    wcTags.generate_from_frequencies(TuplelistTag)\n",
    "    axarr[2].imshow(wcTags.recolor(color_func=color_func, random_state=3))\n",
    "    axarr[2].axis(\"off\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BoWsDict = {}\n",
    "BoRDict = {}\n",
    "BoHDict = {}\n",
    "# import the bag of words from each party\n",
    "import time\n",
    "start_time = time.time()\n",
    "loadInSepOcc(BoWsDict, BoRDict, BoHDict, \"politiciansWordCount/\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop words in both dansih and english\n",
    "stopWords = (\"mr,na,er,om,rt,i,af,alle,andet,andre,at,begge,da,de,\" + \n",
    "            \"den,denne,der,deres,det,dette,dig,din,dog,du,ej,eller, \" +\n",
    "            \"en,end,ene,eneste,enhver,et,fem,fire,flere,fleste,for,fordi,\"+\n",
    "            \"forrige,fra,få,før,god,han,hans,har,hendes,her,hun,hvad,hvem,\"+\n",
    "            \"hver,hvilken,hvis,hvor,hvordan,hvorfor,hvornår,i,ikke,ind,ingen\"+\n",
    "            \",intet,jeg,jeres,kan,kom,kommer,lav,lidt,lille,man,mand,mange,med\"+\n",
    "            \",meget,men,mens,mere,mig,ned,ni,nogen,noget,ny,nyt,nær,næste,\"+\n",
    "            \"næsten,og,op,otte,over,på,se,seks,ses,som,stor,store,syv,ti,til,\"+\n",
    "            \"to,tre,ud,var\").split(',')\n",
    "stopWords = set(stopWords)\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the tf-idf of the top 400 words from each party\n",
    "for party in parties:\n",
    "    plotWordCloud(convertToLoT(calcTfIdf(BoWsDict[party], BoWsDict, 400)), \n",
    "                  convertToLoT(calcTfIdf(BoRDict[party], BoRDict, 400)),\n",
    "                  convertToLoT(calcTfIdf(BoHDict[party], BoHDict, 400)),\n",
    "                  party)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see then many of the words that are scored the highest by the tf-idf algorithm are either words that are descriptive of the politicial focus areas for the given party. Such as border control (in danish _grænsekontrol_) for the right wing, nationalist conservative parti Dansk Folkeparti or green (in dansih _grøn_) for the left wing Alternativet, probably refering to green energy.\n",
    "\n",
    "But what is also interesting is that parties tend to refer much to themselves, their party members, and events tied to their party, more than they refer to other parties, party members or events.\n",
    "We will be covering this in further detail in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who are they talking about?\n",
    "In this section we will using graph tools and theory to analyze who politicians generally talk about.\n",
    "\n",
    "The following are the top 5 politicians that are the most central in the network are, with regards to how much they are referred to, how much they refer to others and how much they link different communities together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inCentral = nx.in_degree_centrality(G)\n",
    "# find the politician with highest in degree centrality\n",
    "print \"Top 5 Most refered to: \" , sorted(inCentral, key=lambda i: inCentral[i], reverse=True)[:5]\n",
    "\n",
    "outCentral = nx.out_degree_centrality(G)\n",
    "# find the politician with highest out degree centrality\n",
    "print \"Top 5 Most referring: \" ,sorted(outCentral, key=lambda i: outCentral[i], reverse=True)[:5]\n",
    "\n",
    "between = nx.betweenness_centrality(multiG)\n",
    "# find the politician with highest betweeness centrality\n",
    "print \"Top 5 Most linking: \" ,sorted(between, key=lambda i: between[i], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assortativity\n",
    "By calculating the assortativity in relation to degree on the multi graph, we can see that polticians that are often refered to, tend to refer to each other, while politicians that are not often referred to tend to refer to each other.\n",
    "This is not surprising news, as social networks tend to have these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assortativity = nx.degree_pearson_correlation_coefficient(multiG)\n",
    "print assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more interesting is to calculate the assortativity according to their index on the political left to right spectrum. We have indexed all party members according to their party's place in the order of left to right, found on [this](http://www.altinget.dk/artikel/det-nye-politiske-kompas) website. The scale has been developed by Kenneth Thue Nielsen and are based on numbers from the research agency [Norstat](http://www.norstat.dk/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assortativity = nx.attribute_assortativity_coefficient(multiG, 'politicalSpectrum')\n",
    "print assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicating that politicians tend to reference other politicians that are within the same political spectrum as themselves. \n",
    "\n",
    "If we calculate the assortativity according to which political block they are part of, we can see that the assortivity is also quite high but lower than that of the polticial spectrum.\n",
    "\n",
    "By _block_ we refer to whether a party is said to belong to the red or blue block, depending on if they tend to vote for the left wing or the right wing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assortativity = nx.attribute_assortativity_coefficient(multiG, 'Block')\n",
    "print assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity\n",
    "By calculating the modularity, we can see whether the parties are good distributions of communities according to who they reference. This however, cannot be performed on multi graphs or directed graphs. To accomodate that the simple directed graph aggregates references, we have fitted the edges with weights to indicate that a reference has occured several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to undirected\n",
    "Gun = G.to_undirected()\n",
    "# get the parties of the polticians\n",
    "politiciansPartyDict = nx.get_node_attributes(Gun, \"party\")\n",
    "# calculate the best partition of the parties\n",
    "partitions = community.best_partition(Gun, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print community.modularity(politiciansPartyDict, Gun, weight='weight')\n",
    "print community.modularity(partitions, Gun, weight='weight')\n",
    "print len(set(partitions.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing that the distribution of communities based on parties, does not defer that much from the optimal destribution, which would be to seperate the parties into 20 different parties. If we try to calculate the optimal destribution while keeping the original amount of parties we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitions = community.best_partition(Gun, weight='weight', partition=politiciansPartyDict)\n",
    "print community.modularity(partitions, Gun, weight='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can than make a confusion matrix filled with percentages to visualize what percentages of which parties should be redistributed into new parties to construct the optimal destribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prctMatrix = []\n",
    "for c in set(partitions.values()):\n",
    "    counts = []\n",
    "    for party in parties:\n",
    "        count = 0\n",
    "        nodesInParty = [node for node,attr in politiciansPartyDict.items() if attr == party ]\n",
    "        nodesInPartition = [node for node,attr in partitions.items() if attr == c ]\n",
    "        for node in nodesInPartition:\n",
    "            if node in nodesInParty:\n",
    "                count += 1\n",
    "        counts.append(float(count)/len(nodesInParty) * 100)\n",
    "        \n",
    "    prctMatrix.append(counts)\n",
    "\n",
    "columnNames = parties\n",
    "Dprct = pd.DataFrame(prctMatrix, columns= columnNames)\n",
    "Dprct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is ofcourse solely based on who politicians of each party tend to reference, and therefore sais nothing about what parties should be formed based on political views. It does however tell us that politicians tend to refer to polticians whithin their own party more than they refer to politicians that are part of other parties. This seems quite positive, and in the following section we will be covering how tend to tweet in general and about eachother.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What is their mood?\n",
    "In this section we will explore the sentiment valence of words written by politicians on Twitter. \n",
    "We will look into what it looks like over time as well as what it looks like when the politicians reference each other in a tweet.\n",
    "\n",
    "We have used [MRJob](https://github.com/Yelp/mrjob) written by Yelp, which is a MapReduce library for Python to calculate the sentiment of a tweet and aggregate it on party by party basis. The code for our MapReduce job can be found in [Appendix E](#Appendix-E).\n",
    "\n",
    "Here we import the files and you can see that there are many _NaN_ values which is because of how we join the files together into a DataFrame. There are fewer and fewer _NaN_ values over time when the politicians of the parties become more active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiDf = pd.DataFrame()\n",
    "for party in parties:\n",
    "    senti = pd.read_csv('partier/sentiment_'+party+'.tsv', \n",
    "                         sep='\\t',\n",
    "                         parse_dates=[0],\n",
    "                         header=None, \n",
    "                         names = [\"Date\", party],\n",
    "                         index_col=0\n",
    "                        )\n",
    "    sentiDf = sentiDf.join(senti, how='outer')\n",
    "sentiDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below plot you can see the number of _NaN_ values over time just to get a feeling with how active the different parties are on Twitter. The y-axis corresponds to how many _NaN_ values there are per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiDf.isnull().sum(axis=1).rolling(window=10).mean().plot(figsize=(15,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like halfway through 2014 might be a good date to perform the rest of the analysis on as the activity is picking up there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(9, sharex=True, sharey=True, figsize=(15,12))\n",
    "count = 0\n",
    "plt.tight_layout(h_pad=1.2)\n",
    "formatter = DateFormatter('%y-%m-%d')\n",
    "for party in parties:\n",
    "    axarr[count].set_title(party, fontsize = 15)\n",
    "    axarr[count].plot(sentiDf['2014-06-01' < sentiDf.index][party].dropna(), c = colorCoding[party])\n",
    "    count += 1\n",
    "    \n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that most party's politicians have become more and more active on Twitter during the last years. Some parties did not have active people on Twitter before 2012.\n",
    "Some had breaks for a long time and the data is generally really sparse before 2013-2014. Which is why we will focus on tweets from 2014 and onwards.\n",
    "\n",
    "It is hard to see what is going on in the plots because of all the noise. Let's take a look at it again where we average over a period. We tried averaging over many intervals but we will only show it for the last 7 days to see if we can see a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(9, sharex=True, sharey=True, figsize=(15,12))\n",
    "count = 0\n",
    "plt.tight_layout(h_pad=1.2)\n",
    "formatter = DateFormatter('%y-%m-%d')\n",
    "for party in parties:\n",
    "    axarr[count].set_title(party, fontsize = 15)\n",
    "    axarr[count].plot(sentiDf['2014-01-01' < sentiDf.index][party].dropna().rolling(window=7, min_periods=0).mean(), c = colorCoding[party])\n",
    "    count += 1\n",
    "    \n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now much easier to see what is happening but we only see it on a 7 day period though. Some patterns we can notice are: all the parties are clearly more positive than they are negative, we can see they hover around 0,6 in average (we calculated it to be 0,633); and there are some tendencies that can be spotted for example a period in July where all the parties seem to have a more positive sentiment value. Some parties look like they might have a high correlation so let's look into that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the correlation between the sentiment of the parties during a **moving window average** of _7 days_ didn't give us much insight as all the values were close to 0. The two highest values were:\n",
    "* _Venstre_ and _Dansk Folkeparti_ = 0,28\n",
    "* _Enhedslisten_ and _Socialistisk Folkeparti_ = 0,26\n",
    "\n",
    "If we average over the last _30 days_ we see more interesting correlations.\n",
    "_Venstre_ and _Dansk Folkeparti_ is now at 0,48, and _Det Konservative Folkeparti_ and _Liberal Alliance_ are those that have the highest negative value of -0,325120.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiDf[sentiDf.index > '2014-01-01'].rolling(window=30, min_periods=0).mean().corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at specific dates which might be important to politicians\n",
    "* 2015-02-14 - A shooting happened at _Krudttønden_ in Denmark.\n",
    "* 2015-04-16 - The queen turned 75 years old.\n",
    "* 2015-09-02 - The first dane (Andreas Mogensen) in space\n",
    "* 2016-08-15 - Danish swimmer wins gold at OL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Queen\", sentiDf['2015-04-16' == sentiDf.index].mean().mean() # queen\n",
    "print \"Shooting\", sentiDf['2015-02-14' == sentiDf.index].mean().mean() # krudttønden\n",
    "print \"Space\", sentiDf['2015-09-02' == sentiDf.index].mean().mean() # dane in space\n",
    "print \"Gold OL\", sentiDf['2016-08-15' == sentiDf.index].mean().mean() # Swimmer wins gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last date which should be polarizing for the parties according to their politics is:\n",
    "* 2015-12-03 - No to discarding the European Union legal reservations (EU retsforbehold)\n",
    "\n",
    "Where we some parties are very happy and others are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print sentiDf['2015-12-03' == sentiDf.index].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are they talking about each other?\n",
    "In the following section we will be covering what sentiment profile polticians have when they refer to politicians from various parties. By embedding each edge with the sentiment of the tweet that referred to another politician, we can calculate what the average sentiment is between all the parties.\n",
    "\n",
    "The following functions are used to calculate the average sentiment for each party towards all other parties, including themselves, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcPartyReferenceSentiment(multiG):\n",
    "    partiesReferenceSentiment = {}\n",
    "    partiesReferenceSentimentCount = {}\n",
    "    for edge in multiG.edges(data=True):\n",
    "        currentParty = multiG.node[edge[0]]['party']\n",
    "        otherParty = multiG.node[edge[1]]['party']\n",
    "        if edge[2]['sentiment'] != None and currentParty in parties and otherParty in parties:\n",
    "            if currentParty in partiesReferenceSentiment:\n",
    "                if otherParty in partiesReferenceSentiment[currentParty]:\n",
    "                    partiesReferenceSentiment[currentParty][otherParty] += edge[2]['sentiment']\n",
    "                    partiesReferenceSentimentCount[currentParty][otherParty] += 1\n",
    "                else:\n",
    "                    partiesReferenceSentiment[currentParty][otherParty] = edge[2]['sentiment']\n",
    "                    partiesReferenceSentimentCount[currentParty][otherParty] = 1\n",
    "            else:\n",
    "                partiesReferenceSentiment[currentParty] = {otherParty:edge[2]['sentiment']}\n",
    "                partiesReferenceSentimentCount[currentParty] =  {otherParty: 1 }\n",
    "    for currentParty in parties:\n",
    "        for otherParty in parties:\n",
    "            partiesReferenceSentiment[currentParty][otherParty] = partiesReferenceSentiment[currentParty][otherParty]/partiesReferenceSentimentCount[currentParty][otherParty]\n",
    "    return partiesReferenceSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partiesReferenceSentiment = calcPartyReferenceSentiment(multiG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotPoliticalPartySentiment(partiesReferenceSentiment):\n",
    "    f, axarr = plt.subplots(3, 3, sharex=True, figsize=(10,18))\n",
    "    countx = 0\n",
    "    county = 0\n",
    "    colors = []\n",
    "    plt.tight_layout(pad=0.4, w_pad=10.5, h_pad=5)\n",
    "    sns.set_style({'xtick.major.size': 0.0})\n",
    "    for party in parties:\n",
    "        axarr[countx][county].set_title(party, fontsize=20)\n",
    "        partyNames = sorted(partiesReferenceSentiment[party], key=lambda i: partiesReferenceSentiment[party][i], reverse=True)\n",
    "        colors = [colorCoding[partyName] for partyName in partyNames]\n",
    "        sentimentValues = sorted(partiesReferenceSentiment[party].values(), reverse=True)\n",
    "        sns.barplot(x=sentimentValues, y=[y.decode('utf-8') for y in partyNames], ax=axarr[countx][county], palette=colors)\n",
    "        countx += 1\n",
    "        if countx % 3 == 0:\n",
    "            county += 1\n",
    "            countx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotPoliticalPartySentiment(partiesReferenceSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based upon these plot there seems to be a small tendency for parties to write more positively about parties they share politicial values with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As we stated ealier in the report it is quite difficult to point what the reasons are of polticial alienation, and the reasons are problably numerous. Our research seems to indicate that it is not by promoting negative rhetoric about political adversaries on twitter that they are inciting a disbelief in politicians. \n",
    "\n",
    "However, our simple sentiment analysis is good when trying to determine wether a text is generally positive or negative, but not good a deciphering complex sentence structure and intricate natural language details. Such as whether a positivite/negativite word is referring to a specific person, or whether a positive word is used sarcasticly.\n",
    "It is therefore not possible from our analysis whether politicians writing about each other in positive terms on twitter.\n",
    "But we can determine that politicians generally talk more about their own party members and party members of fellow minded parties, more than they talk about politicians from parties of the other side of the political spectrum.\n",
    "\n",
    "### Future work\n",
    "The are other natural language processing tools available, which we would like to try. Such as the google natural language processing API, which can deconstruct a text into sentences with individual sentiment profiles. Giving a more detailed picture of the references.\n",
    "\n",
    "Poltiticians have become more active on twitter so doing the same data analysis could interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweetDictionizer\n",
    "reload(tweetDictionizer)\n",
    "ad = AlphabetDetector()\n",
    "\n",
    "for party in parties:\n",
    "    # get names of all the politicians that are part of the party\n",
    "    fileNamesList = fullDf.loc[fullDf['Party'] == party]['twitterAccount']\n",
    "    fileNamesList = [\"cleaned_politikere_tweets/\" + fileName[1:] + \".csv\" for fileName in fileNamesList]\n",
    "    wordDict = {}\n",
    "    \n",
    "\n",
    "    # feed filenames to MRJob script with filepaths\n",
    "    mr_job = tweetDictionizer.MRTweetDictionizer(args=fileNamesList)\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            # collect output from MRJob script\n",
    "            key, value = mr_job.parse_output_line(line)\n",
    "            if not isinstance(key, unicode):\n",
    "                key = key.decode('utf-8')\n",
    "                # key is transformed into unicode \n",
    "                # so that it can be checked wether or not it\n",
    "                # contains letters tha do no not \n",
    "                # belong to the western alphabet\n",
    "            if ad.is_latin(key):\n",
    "                # remove all ',' '.' fro\n",
    "                key = key.encode('utf-8').replace(',', '').replace('.', '')\n",
    "                if not 'http' in key:\n",
    "                    if key not in wordDict:\n",
    "                        wordDict[key] = value \n",
    "                    else:\n",
    "                        wordDict[key] += value\n",
    "\n",
    "\n",
    "    pickle.dump(collections.OrderedDict(sorted(wordDict.iteritems(), \n",
    "                                           key=lambda t: t[1], reverse=True)), \n",
    "            open(\"politiciansWordCount/\" + party + \"WordCount.p\", \"wb\" ) )\n",
    "    \n",
    "    print \"finished \" + party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_moving(text,moving_size):\n",
    "    # initialize array with mean 0.\n",
    "    moving_text = deque([0]*moving_size,maxlen=moving_size) \n",
    "    moving_date = deque(maxlen=moving_size)\n",
    "    values = []\n",
    "    # iterate through the tuples\n",
    "    # and append the text and date\n",
    "    # to two different arrays\n",
    "    for key,value in text.itertuples():\n",
    "        moving_text.append(value)\n",
    "        moving_date.append(key)\n",
    "        # calculate average of date and value\n",
    "        values.append((\n",
    "                np.array(moving_date, dtype='datetime64[D]').view('i8').mean().astype('datetime64[D]'),\n",
    "                np.average(moving_text)))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "FILES=politikere_tweets/*\n",
    "for f in $FILES\n",
    "do\n",
    "  gawk -v RS='\"' 'NR % 2 == 0 { gsub(/\\n/, \"\") } { printf(\"%s%s\", $0, RT) }' $f > \"cleaned_\"$f\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def csvCleaner(fileName, identifyerWord):\n",
    "    with open('cleaned_politikere_tweets/' + fileName, 'rb') as inp, open('cleaned_politikere_tweets/' +'new'+fileName, 'wb') as out:\n",
    "        writer = csv.writer(out)\n",
    "        for row in csv.reader(inp):\n",
    "            if not identifyerWord in row[2]:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how our MRJob script is defined in another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from tweetProtocol import TweetProtocol\n",
    "import re\n",
    "import fileinput\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "class MRSentiment(MRJob):\n",
    "    sentiment = {}\n",
    "    fnames = glob('AFINN-*.txt')\n",
    "    for line in fileinput.FileInput(fnames):\n",
    "        words = line.split('\\t')\n",
    "        if not isinstance(words[0], unicode):\n",
    "            word = words[0].decode('utf-8')\n",
    "        sentiment[word] = words[1]\n",
    "    \n",
    "    INPUT_PROTOCOL = TweetProtocol\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.columnMapper\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.wordMapper,\n",
    "                reducer=self.wordCount\n",
    "            )\n",
    "        ]\n",
    "    def columnMapper(self, _, columns):\n",
    "        try: \n",
    "            yield columns[1][:10], columns[2]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def wordMapper(self, key, tweet):\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            if not isinstance(word, unicode):\n",
    "                word = word.decode('utf-8')\n",
    "            word = re.sub(r\"[,.]\", '', word.lower(), flags=re.U)\n",
    "            if word in self.sentiment:\n",
    "                yield key, float(self.sentiment[word])\n",
    "\n",
    "    def wordCount(self, key, values):\n",
    "        a = []\n",
    "        for value in values:\n",
    "            a.append(value)\n",
    "        yield key, sum(a)/float(len(a))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSentiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how it was called from a notebook to calculate the sentiment of the parties over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweetSentimentOverTime\n",
    "reload(tweetSentimentOverTime)\n",
    "import os\n",
    "\n",
    "for party in set(list(fullDf[pd.notnull(fullDf['Party'])]['Party'])):\n",
    "    partinavn = party\n",
    "    soap = {}\n",
    "    fileNamesList = []\n",
    "    for twitterHandle in list(fullDf[fullDf['Party'] == partinavn]['twitterAccount']):\n",
    "        fileNamesList.append(\"cleaned_politikere_tweets/\"+twitterHandle[1:]+\".csv\")\n",
    "\n",
    "    mr_job = tweetSentimentOverTime.MRSentiment(args=fileNamesList)\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            # collect output from MRJob script\n",
    "            key, value = mr_job.parse_output_line(line)\n",
    "            soap[key] = value\n",
    "    with open ('partier/sentiment_'+partinavn+'.tsv','a') as f:\n",
    "        for k,v in sorted(soap.iteritems()):\n",
    "            f.write(\"{}\\t{}\\n\".format(k, v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
